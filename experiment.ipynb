{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "\n",
    "# Open the image\n",
    "image = Image.open('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/class_data/sample/scratch/Converted_ 0069.png')\n",
    "\n",
    "# Load the bounding box data from the JSON file\n",
    "with open('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/class_data/sample/scratch/Converted_ 0069.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result\n",
    "\n",
    "if data['shapes'] != 1:\n",
    "    for i in range(len(data['shapes'])):\n",
    "        points = data['shapes'][i]['points']\n",
    "        print(points)\n",
    "        x1, y1 = points[0]\n",
    "        x2, y2 = points[1]\n",
    "\n",
    "        # Create an ImageDraw object\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # paint and write the text with the bounding box points\n",
    "        # text\n",
    "        draw.text((x1, y1), \"x1, y1\", fill='red')\n",
    "        draw.text((x2, y2), \"x2, y2\", fill='red')\n",
    "        \n",
    "        # paint the bounding box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline='green')\n",
    "\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        # draw.rectangle([x1, y1, x2, y2], outline='green')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_set_dir = '../defect-detection-and-segment-deep-learning/class_data/Train'\n",
    "test_set_dir = '../defect-detection-and-segment-deep-learning/class_data/Val'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### project target\n",
    "* You are asked to detect and segment the defects of manufacturing from given images. \n",
    "* You are given with  total 450  images and ground truth annotations(mask and bounding box positions).\n",
    "* You can use the image processing skills as well as deep learning methods for this homework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project steps\n",
    "* Step 1: Data preprocessing\n",
    "* Step 2: Data augmentation\n",
    "* Step 3: Model training: dectection and segmentation models\n",
    "* Step 4: Model evaluation with IoU and precision\n",
    "* Step 5: Creat a GUI for defect detection and segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to apply to the images\n",
    "transform = torchvision.transforms.Compose([\n",
    "    # resize the image to 224x224\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    \n",
    "    # convert the rgb image to grayscale\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    # convert the image to a tensor\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Define the custom dataset class\n",
    "class DefectDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, mask:bool=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mask = mask\n",
    "        self.class_names = ['powder_uncover', 'powder_uneven', 'scratch']\n",
    "        self.types = ['image']\n",
    "        self.image_filenames = []\n",
    "        # bounding box\n",
    "        self.boxes = []\n",
    "        self.labels = []\n",
    "        self.mask_filenames = []\n",
    "        for class_name in self.class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            # concatenate the image, label and mask directories\n",
    "            for type_name in self.types:\n",
    "                type_dir = os.path.join(class_dir, type_name)\n",
    "                for filename in os.listdir(type_dir):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_filenames.append(os.path.join(type_dir, filename))\n",
    "                        # read the label file for bounding box position\n",
    "                        with open(os.path.join(type_dir.replace('image', 'label'), filename.replace('.png', '.json'))) as f:\n",
    "                            label_dict = json.load(f)\n",
    "                            for i in range(len(label_dict['shapes'])):\n",
    "                                points = label_dict['shapes'][i]['points']\n",
    "                                x1, y1 = points[0]\n",
    "                                x2, y2 = points[1]\n",
    "                                self.boxes.append([x1, y1, x2, y2])\n",
    "                        self.mask_filenames.append(os.path.join(type_dir.replace('image', 'mask'), filename.replace('.png', '.png')))\n",
    "                        self.labels.append(class_name)\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_filenames[idx])\n",
    "        \n",
    "        \n",
    "        box = self.boxes[idx]\n",
    "        mask = Image.open(self.mask_filenames[idx]).convert('L')\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        if self.mask == True:    \n",
    "            return image, box, mask, label\n",
    "        else:\n",
    "            return image, box, label\n",
    "    \n",
    "# Load the training and test datasets\n",
    "trainset = DefectDetectionDataset(root_dir=train_set_dir, transform=transform)\n",
    "testset = DefectDetectionDataset(root_dir=test_set_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "model = torchvision.models.resnet50()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(trainset.class_names))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:02<00:00, 34.65it/s]\n",
      "100%|██████████| 100/100 [00:02<00:00, 39.65it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 203.44it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 34.99it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 40.65it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 382.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "yolo_train_image_path = \"/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/yolov7/defect/image/train/\"\n",
    "yolo_val_image_path = \"/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/yolov7/defect/image/val/\"\n",
    "yolo_image_path = \"/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/yolov7/defect/\"\n",
    "\n",
    "train_set_dir = '../defect-detection-and-segment-deep-learning/class_data/Train'\n",
    "test_set_dir = '../defect-detection-and-segment-deep-learning/class_data/Val'\n",
    "\n",
    "class_names = ['powder_uncover', 'powder_uneven', 'scratch']\n",
    "types = ['image']\n",
    "yolo_csv = pd.DataFrame(columns=[\"category\", \"x\", \"y\", \"w\", \"h\", \"image_name\", \"image_path\"])\n",
    "\n",
    "# Load the COCO file as a dictionary\n",
    "def convert_to_yolo_format(convert_img_file:str, save_img_file_name:str, save:bool=False):\n",
    "    \n",
    "\n",
    "\n",
    "    label_path = convert_img_file.replace(\"image\", \"label\").replace(\".png\", \".json\")\n",
    "    image_name = convert_img_file.split(\"/\")[-1]\n",
    "    \n",
    "    with open(label_path, \"r\") as file:\n",
    "        json_file = json.load(file)\n",
    "\n",
    "    width, height = Image.open(convert_img_file).size\n",
    "    \n",
    "    for annotation in json_file[\"shapes\"]:\n",
    "        if annotation[\"label\"] == \"powder_uncover\":\n",
    "            category_id = 0\n",
    "        elif annotation[\"label\"] == \"powder_uneven\":\n",
    "            category_id = 1\n",
    "        else:\n",
    "            category_id = 2\n",
    "        points = annotation[\"points\"]\n",
    "        \n",
    "        # normalize the bounding box\n",
    "        x_min, y_min = points[0]\n",
    "        x_max, y_max = points[1]\n",
    "        x = (x_min + (x_max-x_min)/2) * 1.0 / width\n",
    "        y = (y_min + (y_max-y_min)/2) * 1.0 / height\n",
    "        w = (x_max-x_min) * 1.0 / width\n",
    "        h = (y_max-y_min) * 1.0 / height\n",
    "        # print(category_id, x, y, w, h)\n",
    "        \n",
    "        yolo_format_data = str(category_id) + \" \" + str(x) + \" \" + str(y) + \" \" + str(w) + \" \" + str(h)\n",
    "        \n",
    "        yolo_csv.loc[len(yolo_csv)] = [category_id, x, y, w, h, image_name, convert_img_file]\n",
    "        \n",
    "        # save the bounding box and category_id to a text file\n",
    "        # name is the same as the image name\n",
    "        \n",
    "        \n",
    "        \n",
    "        if save == True:\n",
    "        \n",
    "            with open(yolo_train_image_path.replace(\"image\", \"labels\") + image_name.replace(\".png\", \".txt\"), \"a\") as file:\n",
    "                file.write(yolo_format_data)\n",
    "                file.write(\"\\n\")\n",
    "                \n",
    "    if save == True:\n",
    "    # save the image path to a text file\n",
    "        with open(yolo_image_path + f\"{ save_img_file_name }.txt\", \"a\") as file:\n",
    "            file.write(f\"./images/{ save_img_file_name }/\" + image_name)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "sample_folder = \"/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/class_data/sample/\"\n",
    "\n",
    "for dataset in [train_set_dir, test_set_dir]:\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(dataset, class_name)\n",
    "        # concatenate the image, label and mask directories\n",
    "        for type_name in types:\n",
    "            type_dir = os.path.join(class_dir, type_name)\n",
    "            for filename in tqdm(os.listdir(type_dir)):\n",
    "                # rename the image name with class + image name\n",
    "\n",
    "                # Image.open(f\"{type_dir}/{filename}\").save(f\"{type_dir}/{class_name}_{filename}\")\n",
    "                \n",
    "                if dataset == train_set_dir:\n",
    "                    convert_to_yolo_format(convert_img_file=f\"{type_dir}/{filename}\", save_img_file_name='train')\n",
    "                else:\n",
    "                    convert_to_yolo_format(convert_img_file=f\"{type_dir}/{filename}\", save_img_file_name='val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['../defect-detection-and-segment-deep-learning/class_data/Val/powder_uncover/image/converted_ 0395.png'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_csv.query(\"image_path.str.contains('0395.png')\").image_path.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(\"../defect-detection-and-segment-deep-learning/class_data/Val/powder_uncover/image/converted_ 0395.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>converted_ 0129.png</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0128.png</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0395.png</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0137.png</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0141.png</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0206.png</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0205.png</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0204.png</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0203.png</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converted_ 0116.png</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     category   x   y   w   h  image_path\n",
       "image_name                                               \n",
       "converted_ 0129.png        13  13  13  13  13          13\n",
       "converted_ 0128.png        12  12  12  12  12          12\n",
       "converted_ 0395.png         9   9   9   9   9           9\n",
       "converted_ 0137.png         8   8   8   8   8           8\n",
       "converted_ 0141.png         7   7   7   7   7           7\n",
       "...                       ...  ..  ..  ..  ..         ...\n",
       "converted_ 0206.png         1   1   1   1   1           1\n",
       "converted_ 0205.png         1   1   1   1   1           1\n",
       "converted_ 0204.png         1   1   1   1   1           1\n",
       "converted_ 0203.png         1   1   1   1   1           1\n",
       "converted_ 0116.png         1   1   1   1   1           1\n",
       "\n",
       "[375 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yolo_csv.groupby(\"image_name\").count().sort_values(by=\"image_path\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('dip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95682e3b17397ec86f7197f5db473d6bf5c108e5e4327ec68d94866d6630a2c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
