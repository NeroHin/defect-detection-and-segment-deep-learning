{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import json\n",
    "\n",
    "\n",
    "image_folder_path = '/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/test/image/'\n",
    "image_name_list = [image_folder_path + img_path for img_path in os.listdir(image_folder_path) if img_path.endswith('.png')]\n",
    "image_name_list.sort()\n",
    "\n",
    "label_folder_path = '/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/test/label/'\n",
    "label_name_list = [label_folder_path + label_path for label_path in os.listdir(label_folder_path) if label_path.endswith('.json')]\n",
    "label_name_list.sort()\n",
    "\n",
    "bbox_folder_path = '/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/test/bbox/'\n",
    "\n",
    "def draw_and_save_bbox(image_path:str, bbox_label:str, save_image:bool=False):\n",
    "\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Load the bounding box data from the JSON file\n",
    "    with open(bbox_label) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if data['shapes'] != 1:\n",
    "        for i in range(len(data['shapes'])):\n",
    "            points = data['shapes'][i]['points']\n",
    "\n",
    "            x1, y1 = points[0]\n",
    "            x2, y2 = points[1]\n",
    "            \n",
    "            # convert to yolo format\n",
    "            \n",
    "            # Create an ImageDraw object\n",
    "            draw = ImageDraw.Draw(image)\n",
    "\n",
    "            \n",
    "            # paint the bounding box\n",
    "            draw.rectangle([x1, y1, x2, y2], outline='green', width=7)\n",
    "            \n",
    "            # filename\n",
    "            filename = image_path.split('/')[-1]\n",
    "            \n",
    "            if save_image == True:\n",
    "            # save image\n",
    "                image.save(bbox_folder_path + filename.replace('.png', '_bbox.png'))\n",
    "\n",
    "for i in range(len(image_name_list)):\n",
    "    print(image_name_list[i], label_name_list[i])\n",
    "    draw_and_save_bbox(image_name_list[i], label_name_list[i], save_image=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name_list.sort()\n",
    "image_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_list.sort()\n",
    "label_name_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### project target\n",
    "* You are asked to detect and segment the defects of manufacturing from given images. \n",
    "* You are given with  total 450  images and ground truth annotations(mask and bounding box positions).\n",
    "* You can use the image processing skills as well as deep learning methods for this homework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project steps\n",
    "* Step 1: Data preprocessing\n",
    "* Step 2: Data augmentation\n",
    "* Step 3: Model training: dectection and segmentation models\n",
    "* Step 4: Model evaluation with IoU and precision\n",
    "* Step 5: Creat a GUI for defect detection and segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import  tqdm\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_set_dir = '../defect-detection-and-segment-deep-learning/class_data/Train'\n",
    "test_set_dir = '../defect-detection-and-segment-deep-learning/class_data/Val'\n",
    "\n",
    "# Define the transformations to apply to the images\n",
    "transform = torchvision.transforms.Compose([\n",
    "    # resize the image to 224x224\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    \n",
    "    # convert the rgb image to grayscale\n",
    "    torchvision.transforms.Grayscale(num_output_channels=1),\n",
    "    # convert the image to a tensor\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Define the custom dataset class\n",
    "class DefectDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, mask:bool=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mask = mask\n",
    "        self.class_names = ['powder_uncover', 'powder_uneven', 'scratch']\n",
    "        self.types = ['image']\n",
    "        self.image_filenames = []\n",
    "        self.mask_filenames = []\n",
    "        self.bbox_filenames = []\n",
    "        for class_name in tqdm(self.class_names):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            # concatenate the image, label and mask directories\n",
    "            for type_name in self.types:\n",
    "                type_dir = os.path.join(class_dir, type_name)\n",
    "                for filename in os.listdir(type_dir):\n",
    "                    if filename.endswith('.png'):\n",
    "                        self.image_filenames.append(os.path.join(type_dir, filename))\n",
    "                        self.mask_filenames.append(os.path.join(type_dir.replace('image', 'mask'), filename.replace('.png', '.png')))\n",
    "                        self.bbox_filenames.append(os.path.join(type_dir.replace('image', 'label'), filename.replace('.png', '.json')))\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_filenames[idx])\n",
    "        mask = Image.open(self.mask_filenames[idx]).convert('L')\n",
    "\n",
    "        with open(self.bbox_filenames[idx]) as f:\n",
    "            bbox_data = json.load(f)\n",
    "\n",
    "        for data in bbox_data['shapes']:\n",
    "            \n",
    "\n",
    "            # Extract the bounding box coordinates from the data\n",
    "            x1, y1 = bbox_data['shapes']['points'][0]\n",
    "            x2, y2 = bbox_data['shapes']['points'][1]\n",
    "\n",
    "            # Convert the bounding box coordinates to relative values between 0 and 1\n",
    "            img_width, img_height = image.size\n",
    "            x1 /= img_width\n",
    "            y1 /= img_height\n",
    "            x2 /= img_width\n",
    "            y2 /= img_height\n",
    "\n",
    "        # Create the bounding box tuple\n",
    "        bbox = (x1, y1, x2, y2)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        if self.mask:\n",
    "            return image, mask, bbox\n",
    "        else:\n",
    "            return image, bbox\n",
    "    \n",
    "# Load the training and test datasets\n",
    "trainset = DefectDetectionDataset(root_dir=train_set_dir, transform=transform)\n",
    "testset = DefectDetectionDataset(root_dir=test_set_dir, transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "model = torchvision.models.resnet50()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, len(trainset.class_names))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['powder_uncover', 'powder_uneven', 'scratch']\n",
    "types = ['image']\n",
    "image_filenames = []\n",
    "mask_filenames = []\n",
    "bbox_filenames = []\n",
    "root_dir = '../defect-detection-and-segment-deep-learning/class_data/Train'\n",
    "\n",
    "\n",
    "for class_name in tqdm(class_names):\n",
    "    class_dir = os.path.join(root_dir, class_name)\n",
    "    # concatenate the image, label and mask directories\n",
    "    for type_name in types:\n",
    "        type_dir = os.path.join(class_dir, type_name)\n",
    "        for filename in os.listdir(type_dir):\n",
    "            if filename.endswith('.png'):\n",
    "                image_filenames.append(os.path.join(type_dir, filename))\n",
    "                mask_filenames.append(os.path.join(type_dir.replace('image', 'mask'), filename.replace('.png', '.png')))\n",
    "                bbox_filenames.append(os.path.join(type_dir.replace('image', 'label'), filename.replace('.png', '.json')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_filenames[0]\n",
    "\n",
    "with open(bbox_filenames[0], \"r\") as file:\n",
    "    data = json.load(file)\n",
    "    print(data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1244, 1254)\n",
      "(1244, 1254)\n",
      "1.039\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# copy \n",
    "\n",
    "img_path = '/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/pytorch-unet/output/scratch_converted_3152.png'\n",
    "mask_path = '/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/defect-detection-and-segment-deep-learning/class_data/Val/scratch/mask/scratch_converted_3152.png'\n",
    "\n",
    "img = cv2.imread(img_path,cv2.IMREAD_GRAYSCALE)\n",
    "# resize with .resize((1254, 1244))\n",
    "mask = cv2.imread(mask_path,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "\n",
    "print(img.shape)\n",
    "print(mask.shape)\n",
    "\n",
    "def dice_coef(y_true:np.ndarray, y_pred:np.ndarray):\n",
    "    smooth = 1\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "\n",
    "    return round((2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth),3)\n",
    "\n",
    "print(dice_coef(y_true=mask, y_pred=img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "test = Image.open('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/data/masks/powder_uncover_converted_0126.png').resize((1254, 1244))\n",
    "\n",
    "\n",
    "test.save('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/data/masks/powder_uncover_converted_0126.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print img size in img_list\n",
    "for img in tqdm(img_list):\n",
    "    print(img)\n",
    "    img = cv2.imread(os.path.join(img_path, img))\n",
    "    if img.shape != (1244, 1254, 3):\n",
    "        print(img.shape)\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# resize mask\n",
    "\n",
    "for img in tqdm(mask_list):\n",
    "    img_file = Image.open(os.path.join(mask_path, img)).resize((1254, 1244))\n",
    "    img_file.save(os.path.join(mask_path, img))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "for mask in tqdm(mask_list):\n",
    "    mask_img = Image.open(os.path.join(mask_path, mask))\n",
    "    mask_img = mask_img.convert('L')\n",
    "    \n",
    "    threshold = 10\n",
    "\n",
    "\n",
    "    table = []\n",
    "    for i in range(256):\n",
    "        if i < threshold:\n",
    "            table.append(0)\n",
    "        else:\n",
    "            table.append(1)\n",
    "    photo = mask_img.point(table, '1')\n",
    "    photo.save(os.path.join(mask_path, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图片二值化\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# img = Image.open('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/data/imgs/scratch_converted_3186.png')\n",
    "img = Image.open('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/data/imgs/powder_uneven_converted_0228.png')\n",
    "print(Img.mode)\n",
    "# 模式L”为灰色图像，它的每个像素用8个bit表示，0表示黑，255表示白，其他数字表示不同的灰度。\n",
    "Img = img.convert('L')\n",
    "\n",
    "plt.imshow(Img)\n",
    "# 自定义灰度界限，大于这个值为黑色，小于这个值为白色\n",
    "# threshold = 10\n",
    "\n",
    "  \n",
    "# table = []\n",
    "# for i in range(256):\n",
    "#   if i < threshold:\n",
    "#     table.append(0)\n",
    "#   else:\n",
    "#     table.append(1)\n",
    "  \n",
    "# # 图片二值化\n",
    "# photo = Img.point(table, '1')\n",
    "# # photo.save('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/data/masks/scratch_converted_3186.png')\n",
    "# plt.imshow(photo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "\n",
    "img = Image.open('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/power_uncover_converted_0126.png')\n",
    "img = img.convert('L')\n",
    "# save the image\n",
    "img.save('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/power_uncover_converted_0126.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/media/buslab/bed7bcae-c46d-4bde-874d-bdeb04d5dec9/NERO/DIP/final_project/Pytorch-UNet/imgs copy/power_uneven_converted_0124.png').size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "input_img = cv2.imread('powder_uneven_converted_0199.png',  cv2.IMREAD_GRAYSCALE)\n",
    "output_img =  cv2.imread('powder_uneven_converted_0199_out.png',  cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "print(type(input_img))\n",
    "\n",
    "def dice_coef(y_true:np.ndarray, y_pred:np.ndarray):\n",
    "    smooth = 1\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_coef(y_true=input_img, y_pred=output_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def thresholding(img:np.ndarray, threshold:int):\n",
    "    \n",
    "    # convert to grayscale\n",
    "    mask_img = img.convert('L')\n",
    "\n",
    "    img_table = []\n",
    "    \n",
    "    # thresholding\n",
    "    for i in range(256):\n",
    "        if i < threshold:\n",
    "            img_table.append(0)\n",
    "        else:\n",
    "            img_table.append(1)\n",
    "\n",
    "    return mask_img.point(table, '1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('dip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95682e3b17397ec86f7197f5db473d6bf5c108e5e4327ec68d94866d6630a2c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
